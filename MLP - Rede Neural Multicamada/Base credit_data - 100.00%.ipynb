{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09fb4e8f",
   "metadata": {},
   "source": [
    "#### Carregando a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66f0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r'../_Material/Bases de dados/Bases pre-processadas/base_credit_data.pkl', mode = 'rb') as arquivo:\n",
    "    x_previsores, y_classes, x_teste, y_teste = pickle.load( arquivo )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab29397e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_previsores.shape, y_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa33b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_teste.shape, y_teste.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e10fee",
   "metadata": {},
   "source": [
    "#### Aplicação do aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e08de920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.42422329\n",
      "Iteration 2, loss = 0.41836486\n",
      "Iteration 3, loss = 0.41308547\n",
      "Iteration 4, loss = 0.40862466\n",
      "Iteration 5, loss = 0.40416398\n",
      "Iteration 6, loss = 0.40044398\n",
      "Iteration 7, loss = 0.39696176\n",
      "Iteration 8, loss = 0.39364486\n",
      "Iteration 9, loss = 0.39060084\n",
      "Iteration 10, loss = 0.38770699\n",
      "Iteration 11, loss = 0.38495022\n",
      "Iteration 12, loss = 0.38233026\n",
      "Iteration 13, loss = 0.37963635\n",
      "Iteration 14, loss = 0.37712173\n",
      "Iteration 15, loss = 0.37456872\n",
      "Iteration 16, loss = 0.37204276\n",
      "Iteration 17, loss = 0.36957077\n",
      "Iteration 18, loss = 0.36706911\n",
      "Iteration 19, loss = 0.36455113\n",
      "Iteration 20, loss = 0.36205668\n",
      "Iteration 21, loss = 0.35955324\n",
      "Iteration 22, loss = 0.35698353\n",
      "Iteration 23, loss = 0.35449746\n",
      "Iteration 24, loss = 0.35182644\n",
      "Iteration 25, loss = 0.34928446\n",
      "Iteration 26, loss = 0.34666047\n",
      "Iteration 27, loss = 0.34403468\n",
      "Iteration 28, loss = 0.34140711\n",
      "Iteration 29, loss = 0.33869709\n",
      "Iteration 30, loss = 0.33598775\n",
      "Iteration 31, loss = 0.33327456\n",
      "Iteration 32, loss = 0.33052335\n",
      "Iteration 33, loss = 0.32778254\n",
      "Iteration 34, loss = 0.32504688\n",
      "Iteration 35, loss = 0.32232202\n",
      "Iteration 36, loss = 0.31963218\n",
      "Iteration 37, loss = 0.31691947\n",
      "Iteration 38, loss = 0.31422889\n",
      "Iteration 39, loss = 0.31159067\n",
      "Iteration 40, loss = 0.30888700\n",
      "Iteration 41, loss = 0.30622974\n",
      "Iteration 42, loss = 0.30360156\n",
      "Iteration 43, loss = 0.30100859\n",
      "Iteration 44, loss = 0.29830182\n",
      "Iteration 45, loss = 0.29579855\n",
      "Iteration 46, loss = 0.29315661\n",
      "Iteration 47, loss = 0.29053829\n",
      "Iteration 48, loss = 0.28802722\n",
      "Iteration 49, loss = 0.28543894\n",
      "Iteration 50, loss = 0.28284636\n",
      "Iteration 51, loss = 0.28024569\n",
      "Iteration 52, loss = 0.27763859\n",
      "Iteration 53, loss = 0.27504617\n",
      "Iteration 54, loss = 0.27247324\n",
      "Iteration 55, loss = 0.26972731\n",
      "Iteration 56, loss = 0.26700543\n",
      "Iteration 57, loss = 0.26431254\n",
      "Iteration 58, loss = 0.26155035\n",
      "Iteration 59, loss = 0.25871785\n",
      "Iteration 60, loss = 0.25596932\n",
      "Iteration 61, loss = 0.25326353\n",
      "Iteration 62, loss = 0.25058975\n",
      "Iteration 63, loss = 0.24800035\n",
      "Iteration 64, loss = 0.24520780\n",
      "Iteration 65, loss = 0.24255865\n",
      "Iteration 66, loss = 0.23990095\n",
      "Iteration 67, loss = 0.23700298\n",
      "Iteration 68, loss = 0.23426342\n",
      "Iteration 69, loss = 0.23130696\n",
      "Iteration 70, loss = 0.22850531\n",
      "Iteration 71, loss = 0.22560530\n",
      "Iteration 72, loss = 0.22273982\n",
      "Iteration 73, loss = 0.21974419\n",
      "Iteration 74, loss = 0.21685998\n",
      "Iteration 75, loss = 0.21385560\n",
      "Iteration 76, loss = 0.21088114\n",
      "Iteration 77, loss = 0.20779329\n",
      "Iteration 78, loss = 0.20492093\n",
      "Iteration 79, loss = 0.20211591\n",
      "Iteration 80, loss = 0.19937908\n",
      "Iteration 81, loss = 0.19665161\n",
      "Iteration 82, loss = 0.19402668\n",
      "Iteration 83, loss = 0.19141066\n",
      "Iteration 84, loss = 0.18875243\n",
      "Iteration 85, loss = 0.18619200\n",
      "Iteration 86, loss = 0.18346994\n",
      "Iteration 87, loss = 0.18071205\n",
      "Iteration 88, loss = 0.17810452\n",
      "Iteration 89, loss = 0.17526688\n",
      "Iteration 90, loss = 0.17218939\n",
      "Iteration 91, loss = 0.16902533\n",
      "Iteration 92, loss = 0.16569911\n",
      "Iteration 93, loss = 0.16261090\n",
      "Iteration 94, loss = 0.15935230\n",
      "Iteration 95, loss = 0.15632966\n",
      "Iteration 96, loss = 0.15309617\n",
      "Iteration 97, loss = 0.15010344\n",
      "Iteration 98, loss = 0.14695936\n",
      "Iteration 99, loss = 0.14398829\n",
      "Iteration 100, loss = 0.14112113\n",
      "Iteration 101, loss = 0.13824913\n",
      "Iteration 102, loss = 0.13576933\n",
      "Iteration 103, loss = 0.13326221\n",
      "Iteration 104, loss = 0.13070832\n",
      "Iteration 105, loss = 0.12826602\n",
      "Iteration 106, loss = 0.12612410\n",
      "Iteration 107, loss = 0.12395755\n",
      "Iteration 108, loss = 0.12217709\n",
      "Iteration 109, loss = 0.12051763\n",
      "Iteration 110, loss = 0.11919243\n",
      "Iteration 111, loss = 0.11775275\n",
      "Iteration 112, loss = 0.11657474\n",
      "Iteration 113, loss = 0.11547774\n",
      "Iteration 114, loss = 0.11430721\n",
      "Iteration 115, loss = 0.11332828\n",
      "Iteration 116, loss = 0.11237871\n",
      "Iteration 117, loss = 0.11145460\n",
      "Iteration 118, loss = 0.11055414\n",
      "Iteration 119, loss = 0.11007080\n",
      "Iteration 120, loss = 0.10900581\n",
      "Iteration 121, loss = 0.10813554\n",
      "Iteration 122, loss = 0.10758272\n",
      "Iteration 123, loss = 0.10675228\n",
      "Iteration 124, loss = 0.10611380\n",
      "Iteration 125, loss = 0.10537100\n",
      "Iteration 126, loss = 0.10469017\n",
      "Iteration 127, loss = 0.10413378\n",
      "Iteration 128, loss = 0.10350132\n",
      "Iteration 129, loss = 0.10266520\n",
      "Iteration 130, loss = 0.10220340\n",
      "Iteration 131, loss = 0.10161124\n",
      "Iteration 132, loss = 0.10099579\n",
      "Iteration 133, loss = 0.10037321\n",
      "Iteration 134, loss = 0.09978240\n",
      "Iteration 135, loss = 0.09922200\n",
      "Iteration 136, loss = 0.09863180\n",
      "Iteration 137, loss = 0.09814789\n",
      "Iteration 138, loss = 0.09753424\n",
      "Iteration 139, loss = 0.09704877\n",
      "Iteration 140, loss = 0.09658052\n",
      "Iteration 141, loss = 0.09610629\n",
      "Iteration 142, loss = 0.09556641\n",
      "Iteration 143, loss = 0.09516088\n",
      "Iteration 144, loss = 0.09460501\n",
      "Iteration 145, loss = 0.09415527\n",
      "Iteration 146, loss = 0.09372026\n",
      "Iteration 147, loss = 0.09331218\n",
      "Iteration 148, loss = 0.09291692\n",
      "Iteration 149, loss = 0.09260663\n",
      "Iteration 150, loss = 0.09203631\n",
      "Iteration 151, loss = 0.09165440\n",
      "Iteration 152, loss = 0.09132002\n",
      "Iteration 153, loss = 0.09084261\n",
      "Iteration 154, loss = 0.09042988\n",
      "Iteration 155, loss = 0.09006338\n",
      "Iteration 156, loss = 0.08969418\n",
      "Iteration 157, loss = 0.08929421\n",
      "Iteration 158, loss = 0.08899239\n",
      "Iteration 159, loss = 0.08856582\n",
      "Iteration 160, loss = 0.08825520\n",
      "Iteration 161, loss = 0.08790169\n",
      "Iteration 162, loss = 0.08752099\n",
      "Iteration 163, loss = 0.08722988\n",
      "Iteration 164, loss = 0.08676383\n",
      "Iteration 165, loss = 0.08646417\n",
      "Iteration 166, loss = 0.08611809\n",
      "Iteration 167, loss = 0.08576915\n",
      "Iteration 168, loss = 0.08546576\n",
      "Iteration 169, loss = 0.08512189\n",
      "Iteration 170, loss = 0.08487485\n",
      "Iteration 171, loss = 0.08451056\n",
      "Iteration 172, loss = 0.08421059\n",
      "Iteration 173, loss = 0.08386605\n",
      "Iteration 174, loss = 0.08354784\n",
      "Iteration 175, loss = 0.08325098\n",
      "Iteration 176, loss = 0.08288632\n",
      "Iteration 177, loss = 0.08260963\n",
      "Iteration 178, loss = 0.08225008\n",
      "Iteration 179, loss = 0.08201362\n",
      "Iteration 180, loss = 0.08170333\n",
      "Iteration 181, loss = 0.08143232\n",
      "Iteration 182, loss = 0.08115222\n",
      "Iteration 183, loss = 0.08081584\n",
      "Iteration 184, loss = 0.08057329\n",
      "Iteration 185, loss = 0.08023230\n",
      "Iteration 186, loss = 0.07995501\n",
      "Iteration 187, loss = 0.07967326\n",
      "Iteration 188, loss = 0.07942070\n",
      "Iteration 189, loss = 0.07910387\n",
      "Iteration 190, loss = 0.07880677\n",
      "Iteration 191, loss = 0.07844398\n",
      "Iteration 192, loss = 0.07819991\n",
      "Iteration 193, loss = 0.07787176\n",
      "Iteration 194, loss = 0.07752587\n",
      "Iteration 195, loss = 0.07737467\n",
      "Iteration 196, loss = 0.07708811\n",
      "Iteration 197, loss = 0.07670306\n",
      "Iteration 198, loss = 0.07636998\n",
      "Iteration 199, loss = 0.07611908\n",
      "Iteration 200, loss = 0.07579343\n",
      "Iteration 201, loss = 0.07553980\n",
      "Iteration 202, loss = 0.07522393\n",
      "Iteration 203, loss = 0.07493458\n",
      "Iteration 204, loss = 0.07459844\n",
      "Iteration 205, loss = 0.07426584\n",
      "Iteration 206, loss = 0.07402760\n",
      "Iteration 207, loss = 0.07369902\n",
      "Iteration 208, loss = 0.07337258\n",
      "Iteration 209, loss = 0.07309484\n",
      "Iteration 210, loss = 0.07284120\n",
      "Iteration 211, loss = 0.07267609\n",
      "Iteration 212, loss = 0.07225229\n",
      "Iteration 213, loss = 0.07201338\n",
      "Iteration 214, loss = 0.07164656\n",
      "Iteration 215, loss = 0.07136985\n",
      "Iteration 216, loss = 0.07111537\n",
      "Iteration 217, loss = 0.07071415\n",
      "Iteration 218, loss = 0.07047454\n",
      "Iteration 219, loss = 0.07010260\n",
      "Iteration 220, loss = 0.06974878\n",
      "Iteration 221, loss = 0.06940290\n",
      "Iteration 222, loss = 0.06908749\n",
      "Iteration 223, loss = 0.06879928\n",
      "Iteration 224, loss = 0.06845962\n",
      "Iteration 225, loss = 0.06811194\n",
      "Iteration 226, loss = 0.06775569\n",
      "Iteration 227, loss = 0.06746639\n",
      "Iteration 228, loss = 0.06714560\n",
      "Iteration 229, loss = 0.06674703\n",
      "Iteration 230, loss = 0.06645851\n",
      "Iteration 231, loss = 0.06609385\n",
      "Iteration 232, loss = 0.06573866\n",
      "Iteration 233, loss = 0.06542402\n",
      "Iteration 234, loss = 0.06509186\n",
      "Iteration 235, loss = 0.06479102\n",
      "Iteration 236, loss = 0.06453598\n",
      "Iteration 237, loss = 0.06415338\n",
      "Iteration 238, loss = 0.06394421\n",
      "Iteration 239, loss = 0.06347246\n",
      "Iteration 240, loss = 0.06322828\n",
      "Iteration 241, loss = 0.06284714\n",
      "Iteration 242, loss = 0.06256054\n",
      "Iteration 243, loss = 0.06222137\n",
      "Iteration 244, loss = 0.06188828\n",
      "Iteration 245, loss = 0.06151984\n",
      "Iteration 246, loss = 0.06119810\n",
      "Iteration 247, loss = 0.06090285\n",
      "Iteration 248, loss = 0.06059900\n",
      "Iteration 249, loss = 0.06032515\n",
      "Iteration 250, loss = 0.06000103\n",
      "Iteration 251, loss = 0.05962112\n",
      "Iteration 252, loss = 0.05934729\n",
      "Iteration 253, loss = 0.05902384\n",
      "Iteration 254, loss = 0.05873178\n",
      "Iteration 255, loss = 0.05842545\n",
      "Iteration 256, loss = 0.05813404\n",
      "Iteration 257, loss = 0.05792874\n",
      "Iteration 258, loss = 0.05758934\n",
      "Iteration 259, loss = 0.05733769\n",
      "Iteration 260, loss = 0.05705791\n",
      "Iteration 261, loss = 0.05677820\n",
      "Iteration 262, loss = 0.05647335\n",
      "Iteration 263, loss = 0.05618145\n",
      "Iteration 264, loss = 0.05601632\n",
      "Iteration 265, loss = 0.05569599\n",
      "Iteration 266, loss = 0.05538907\n",
      "Iteration 267, loss = 0.05513302\n",
      "Iteration 268, loss = 0.05489568\n",
      "Iteration 269, loss = 0.05466474\n",
      "Iteration 270, loss = 0.05437881\n",
      "Iteration 271, loss = 0.05408917\n",
      "Iteration 272, loss = 0.05383212\n",
      "Iteration 273, loss = 0.05362388\n",
      "Iteration 274, loss = 0.05340166\n",
      "Iteration 275, loss = 0.05311403\n",
      "Iteration 276, loss = 0.05278761\n",
      "Iteration 277, loss = 0.05258449\n",
      "Iteration 278, loss = 0.05230771\n",
      "Iteration 279, loss = 0.05205002\n",
      "Iteration 280, loss = 0.05184448\n",
      "Iteration 281, loss = 0.05159937\n",
      "Iteration 282, loss = 0.05131038\n",
      "Iteration 283, loss = 0.05105998\n",
      "Iteration 284, loss = 0.05082204\n",
      "Iteration 285, loss = 0.05056755\n",
      "Iteration 286, loss = 0.05032078\n",
      "Iteration 287, loss = 0.05025583\n",
      "Iteration 288, loss = 0.04991686\n",
      "Iteration 289, loss = 0.04963046\n",
      "Iteration 290, loss = 0.04960626\n",
      "Iteration 291, loss = 0.04917334\n",
      "Iteration 292, loss = 0.04897108\n",
      "Iteration 293, loss = 0.04884230\n",
      "Iteration 294, loss = 0.04861020\n",
      "Iteration 295, loss = 0.04827378\n",
      "Iteration 296, loss = 0.04804968\n",
      "Iteration 297, loss = 0.04787736\n",
      "Iteration 298, loss = 0.04761307\n",
      "Iteration 299, loss = 0.04739776\n",
      "Iteration 300, loss = 0.04725012\n",
      "Iteration 301, loss = 0.04696202\n",
      "Iteration 302, loss = 0.04675427\n",
      "Iteration 303, loss = 0.04659543\n",
      "Iteration 304, loss = 0.04631840\n",
      "Iteration 305, loss = 0.04618195\n",
      "Iteration 306, loss = 0.04596553\n",
      "Iteration 307, loss = 0.04569256\n",
      "Iteration 308, loss = 0.04548322\n",
      "Iteration 309, loss = 0.04532350\n",
      "Iteration 310, loss = 0.04510809\n",
      "Iteration 311, loss = 0.04493101\n",
      "Iteration 312, loss = 0.04469680\n",
      "Iteration 313, loss = 0.04469091\n",
      "Iteration 314, loss = 0.04443378\n",
      "Iteration 315, loss = 0.04408618\n",
      "Iteration 316, loss = 0.04391101\n",
      "Iteration 317, loss = 0.04379998\n",
      "Iteration 318, loss = 0.04351090\n",
      "Iteration 319, loss = 0.04335060\n",
      "Iteration 320, loss = 0.04310950\n",
      "Iteration 321, loss = 0.04296988\n",
      "Iteration 322, loss = 0.04278763\n",
      "Iteration 323, loss = 0.04259901\n",
      "Iteration 324, loss = 0.04244293\n",
      "Iteration 325, loss = 0.04235795\n",
      "Iteration 326, loss = 0.04201769\n",
      "Iteration 327, loss = 0.04185643\n",
      "Iteration 328, loss = 0.04168695\n",
      "Iteration 329, loss = 0.04150972\n",
      "Iteration 330, loss = 0.04137874\n",
      "Iteration 331, loss = 0.04115185\n",
      "Iteration 332, loss = 0.04098735\n",
      "Iteration 333, loss = 0.04083775\n",
      "Iteration 334, loss = 0.04065173\n",
      "Iteration 335, loss = 0.04059531\n",
      "Iteration 336, loss = 0.04044023\n",
      "Iteration 337, loss = 0.04029725\n",
      "Iteration 338, loss = 0.04004692\n",
      "Iteration 339, loss = 0.04003763\n",
      "Iteration 340, loss = 0.03971739\n",
      "Iteration 341, loss = 0.03954121\n",
      "Iteration 342, loss = 0.03941410\n",
      "Iteration 343, loss = 0.03923462\n",
      "Iteration 344, loss = 0.03911107\n",
      "Iteration 345, loss = 0.03906580\n",
      "Iteration 346, loss = 0.03884531\n",
      "Iteration 347, loss = 0.03867462\n",
      "Iteration 348, loss = 0.03849841\n",
      "Iteration 349, loss = 0.03836343\n",
      "Iteration 350, loss = 0.03822519\n",
      "Iteration 351, loss = 0.03817046\n",
      "Iteration 352, loss = 0.03788138\n",
      "Iteration 353, loss = 0.03777375\n",
      "Iteration 354, loss = 0.03763164\n",
      "Iteration 355, loss = 0.03756305\n",
      "Iteration 356, loss = 0.03737921\n",
      "Iteration 357, loss = 0.03721528\n",
      "Iteration 358, loss = 0.03707527\n",
      "Iteration 359, loss = 0.03698245\n",
      "Iteration 360, loss = 0.03685743\n",
      "Iteration 361, loss = 0.03666605\n",
      "Iteration 362, loss = 0.03653643\n",
      "Iteration 363, loss = 0.03637390\n",
      "Iteration 364, loss = 0.03631688\n",
      "Iteration 365, loss = 0.03615007\n",
      "Iteration 366, loss = 0.03598128\n",
      "Iteration 367, loss = 0.03589434\n",
      "Iteration 368, loss = 0.03575210\n",
      "Iteration 369, loss = 0.03560750\n",
      "Iteration 370, loss = 0.03555659\n",
      "Iteration 371, loss = 0.03533333\n",
      "Iteration 372, loss = 0.03518759\n",
      "Iteration 373, loss = 0.03506861\n",
      "Iteration 374, loss = 0.03499138\n",
      "Iteration 375, loss = 0.03479445\n",
      "Iteration 376, loss = 0.03465599\n",
      "Iteration 377, loss = 0.03453864\n",
      "Iteration 378, loss = 0.03440705\n",
      "Iteration 379, loss = 0.03428485\n",
      "Iteration 380, loss = 0.03425629\n",
      "Iteration 381, loss = 0.03397180\n",
      "Iteration 382, loss = 0.03399154\n",
      "Iteration 383, loss = 0.03379531\n",
      "Iteration 384, loss = 0.03360050\n",
      "Iteration 385, loss = 0.03353766\n",
      "Iteration 386, loss = 0.03336427\n",
      "Iteration 387, loss = 0.03325879\n",
      "Iteration 388, loss = 0.03308833\n",
      "Iteration 389, loss = 0.03295761\n",
      "Iteration 390, loss = 0.03282304\n",
      "Iteration 391, loss = 0.03271752\n",
      "Iteration 392, loss = 0.03257694\n",
      "Iteration 393, loss = 0.03256396\n",
      "Iteration 394, loss = 0.03243621\n",
      "Iteration 395, loss = 0.03237367\n",
      "Iteration 396, loss = 0.03219884\n",
      "Iteration 397, loss = 0.03208575\n",
      "Iteration 398, loss = 0.03187315\n",
      "Iteration 399, loss = 0.03172426\n",
      "Iteration 400, loss = 0.03161275\n",
      "Iteration 401, loss = 0.03146629\n",
      "Iteration 402, loss = 0.03134362\n",
      "Iteration 403, loss = 0.03121206\n",
      "Iteration 404, loss = 0.03109552\n",
      "Iteration 405, loss = 0.03102340\n",
      "Iteration 406, loss = 0.03087330\n",
      "Iteration 407, loss = 0.03078198\n",
      "Iteration 408, loss = 0.03068453\n",
      "Iteration 409, loss = 0.03049811\n",
      "Iteration 410, loss = 0.03049674\n",
      "Iteration 411, loss = 0.03025409\n",
      "Iteration 412, loss = 0.03015129\n",
      "Iteration 413, loss = 0.03007525\n",
      "Iteration 414, loss = 0.02995069\n",
      "Iteration 415, loss = 0.02983093\n",
      "Iteration 416, loss = 0.02969036\n",
      "Iteration 417, loss = 0.02960334\n",
      "Iteration 418, loss = 0.02943533\n",
      "Iteration 419, loss = 0.02934638\n",
      "Iteration 420, loss = 0.02923165\n",
      "Iteration 421, loss = 0.02910461\n",
      "Iteration 422, loss = 0.02900243\n",
      "Iteration 423, loss = 0.02888936\n",
      "Iteration 424, loss = 0.02877823\n",
      "Iteration 425, loss = 0.02865943\n",
      "Iteration 426, loss = 0.02855733\n",
      "Iteration 427, loss = 0.02846766\n",
      "Iteration 428, loss = 0.02835076\n",
      "Iteration 429, loss = 0.02832922\n",
      "Iteration 430, loss = 0.02817210\n",
      "Iteration 431, loss = 0.02805909\n",
      "Iteration 432, loss = 0.02797605\n",
      "Iteration 433, loss = 0.02792889\n",
      "Iteration 434, loss = 0.02775281\n",
      "Iteration 435, loss = 0.02761246\n",
      "Iteration 436, loss = 0.02760488\n",
      "Iteration 437, loss = 0.02768498\n",
      "Iteration 438, loss = 0.02738624\n",
      "Iteration 439, loss = 0.02737830\n",
      "Iteration 440, loss = 0.02729611\n",
      "Iteration 441, loss = 0.02710341\n",
      "Iteration 442, loss = 0.02701529\n",
      "Iteration 443, loss = 0.02690092\n",
      "Iteration 444, loss = 0.02680121\n",
      "Iteration 445, loss = 0.02680066\n",
      "Iteration 446, loss = 0.02668594\n",
      "Iteration 447, loss = 0.02666918\n",
      "Iteration 448, loss = 0.02649059\n",
      "Iteration 449, loss = 0.02638376\n",
      "Iteration 450, loss = 0.02633304\n",
      "Iteration 451, loss = 0.02619549\n",
      "Iteration 452, loss = 0.02609827\n",
      "Iteration 453, loss = 0.02604763\n",
      "Iteration 454, loss = 0.02592719\n",
      "Iteration 455, loss = 0.02585969\n",
      "Iteration 456, loss = 0.02577117\n",
      "Iteration 457, loss = 0.02565181\n",
      "Iteration 458, loss = 0.02557323\n",
      "Iteration 459, loss = 0.02554640\n",
      "Iteration 460, loss = 0.02542425\n",
      "Iteration 461, loss = 0.02540922\n",
      "Iteration 462, loss = 0.02521602\n",
      "Iteration 463, loss = 0.02517870\n",
      "Iteration 464, loss = 0.02508883\n",
      "Iteration 465, loss = 0.02498143\n",
      "Iteration 466, loss = 0.02491158\n",
      "Iteration 467, loss = 0.02484280\n",
      "Iteration 468, loss = 0.02477264\n",
      "Iteration 469, loss = 0.02469706\n",
      "Iteration 470, loss = 0.02459288\n",
      "Iteration 471, loss = 0.02451003\n",
      "Iteration 472, loss = 0.02449762\n",
      "Iteration 473, loss = 0.02434716\n",
      "Iteration 474, loss = 0.02429910\n",
      "Iteration 475, loss = 0.02423261\n",
      "Iteration 476, loss = 0.02410726\n",
      "Iteration 477, loss = 0.02404688\n",
      "Iteration 478, loss = 0.02396079\n",
      "Iteration 479, loss = 0.02393277\n",
      "Iteration 480, loss = 0.02382012\n",
      "Iteration 481, loss = 0.02375047\n",
      "Iteration 482, loss = 0.02377984\n",
      "Iteration 483, loss = 0.02363930\n",
      "Iteration 484, loss = 0.02349835\n",
      "Iteration 485, loss = 0.02346041\n",
      "Iteration 486, loss = 0.02335545\n",
      "Iteration 487, loss = 0.02340053\n",
      "Iteration 488, loss = 0.02319489\n",
      "Iteration 489, loss = 0.02314151\n",
      "Iteration 490, loss = 0.02310709\n",
      "Iteration 491, loss = 0.02297404\n",
      "Iteration 492, loss = 0.02292146\n",
      "Iteration 493, loss = 0.02285304\n",
      "Iteration 494, loss = 0.02283910\n",
      "Iteration 495, loss = 0.02267560\n",
      "Iteration 496, loss = 0.02266114\n",
      "Iteration 497, loss = 0.02260196\n",
      "Iteration 498, loss = 0.02248092\n",
      "Iteration 499, loss = 0.02245015\n",
      "Iteration 500, loss = 0.02234061\n",
      "Iteration 501, loss = 0.02227893\n",
      "Iteration 502, loss = 0.02219313\n",
      "Iteration 503, loss = 0.02218311\n",
      "Iteration 504, loss = 0.02210767\n",
      "Iteration 505, loss = 0.02199972\n",
      "Iteration 506, loss = 0.02209636\n",
      "Iteration 507, loss = 0.02194133\n",
      "Iteration 508, loss = 0.02187141\n",
      "Iteration 509, loss = 0.02174378\n",
      "Iteration 510, loss = 0.02168231\n",
      "Iteration 511, loss = 0.02172211\n",
      "Iteration 512, loss = 0.02157137\n",
      "Iteration 513, loss = 0.02151926\n",
      "Iteration 514, loss = 0.02140607\n",
      "Iteration 515, loss = 0.02133767\n",
      "Iteration 516, loss = 0.02130162\n",
      "Iteration 517, loss = 0.02126725\n",
      "Iteration 518, loss = 0.02117502\n",
      "Iteration 519, loss = 0.02109651\n",
      "Iteration 520, loss = 0.02102580\n",
      "Iteration 521, loss = 0.02102649\n",
      "Iteration 522, loss = 0.02093419\n",
      "Iteration 523, loss = 0.02085514\n",
      "Iteration 524, loss = 0.02081247\n",
      "Iteration 525, loss = 0.02073946\n",
      "Iteration 526, loss = 0.02067105\n",
      "Iteration 527, loss = 0.02060723\n",
      "Iteration 528, loss = 0.02057956\n",
      "Iteration 529, loss = 0.02049959\n",
      "Iteration 530, loss = 0.02048316\n",
      "Iteration 531, loss = 0.02038944\n",
      "Iteration 532, loss = 0.02034330\n",
      "Iteration 533, loss = 0.02025570\n",
      "Iteration 534, loss = 0.02020910\n",
      "Iteration 535, loss = 0.02015724\n",
      "Iteration 536, loss = 0.02009458\n",
      "Iteration 537, loss = 0.02004741\n",
      "Iteration 538, loss = 0.02002026\n",
      "Iteration 539, loss = 0.01992490\n",
      "Iteration 540, loss = 0.01989159\n",
      "Iteration 541, loss = 0.01979966\n",
      "Iteration 542, loss = 0.01975634\n",
      "Iteration 543, loss = 0.01969573\n",
      "Iteration 544, loss = 0.01963092\n",
      "Iteration 545, loss = 0.01958930\n",
      "Iteration 546, loss = 0.01956032\n",
      "Iteration 547, loss = 0.01945926\n",
      "Iteration 548, loss = 0.01943747\n",
      "Iteration 549, loss = 0.01938736\n",
      "Iteration 550, loss = 0.01937721\n",
      "Iteration 551, loss = 0.01926461\n",
      "Iteration 552, loss = 0.01921061\n",
      "Iteration 553, loss = 0.01917672\n",
      "Iteration 554, loss = 0.01915222\n",
      "Iteration 555, loss = 0.01905065\n",
      "Iteration 556, loss = 0.01900926\n",
      "Iteration 557, loss = 0.01899737\n",
      "Iteration 558, loss = 0.01893332\n",
      "Iteration 559, loss = 0.01894897\n",
      "Iteration 560, loss = 0.01882761\n",
      "Iteration 561, loss = 0.01875738\n",
      "Iteration 562, loss = 0.01872024\n",
      "Iteration 563, loss = 0.01867881\n",
      "Iteration 564, loss = 0.01860095\n",
      "Iteration 565, loss = 0.01855527\n",
      "Iteration 566, loss = 0.01849782\n",
      "Iteration 567, loss = 0.01846473\n",
      "Iteration 568, loss = 0.01843629\n",
      "Iteration 569, loss = 0.01843680\n",
      "Iteration 570, loss = 0.01834415\n",
      "Iteration 571, loss = 0.01829457\n",
      "Iteration 572, loss = 0.01821672\n",
      "Iteration 573, loss = 0.01823659\n",
      "Iteration 574, loss = 0.01813746\n",
      "Iteration 575, loss = 0.01810765\n",
      "Iteration 576, loss = 0.01805923\n",
      "Iteration 577, loss = 0.01796527\n",
      "Iteration 578, loss = 0.01792722\n",
      "Iteration 579, loss = 0.01790770\n",
      "Iteration 580, loss = 0.01786194\n",
      "Iteration 581, loss = 0.01781441\n",
      "Iteration 582, loss = 0.01774941\n",
      "Iteration 583, loss = 0.01772340\n",
      "Iteration 584, loss = 0.01768023\n",
      "Iteration 585, loss = 0.01763928\n",
      "Iteration 586, loss = 0.01769742\n",
      "Iteration 587, loss = 0.01755260\n",
      "Iteration 588, loss = 0.01754563\n",
      "Iteration 589, loss = 0.01746964\n",
      "Iteration 590, loss = 0.01740436\n",
      "Iteration 591, loss = 0.01737200\n",
      "Iteration 592, loss = 0.01732869\n",
      "Iteration 593, loss = 0.01732524\n",
      "Iteration 594, loss = 0.01726509\n",
      "Iteration 595, loss = 0.01726649\n",
      "Iteration 596, loss = 0.01720643\n",
      "Iteration 597, loss = 0.01715753\n",
      "Iteration 598, loss = 0.01708958\n",
      "Iteration 599, loss = 0.01705880\n",
      "Iteration 600, loss = 0.01699066\n",
      "Iteration 601, loss = 0.01695879\n",
      "Iteration 602, loss = 0.01693056\n",
      "Iteration 603, loss = 0.01685756\n",
      "Iteration 604, loss = 0.01684289\n",
      "Iteration 605, loss = 0.01675697\n",
      "Iteration 606, loss = 0.01675710\n",
      "Iteration 607, loss = 0.01676379\n",
      "Iteration 608, loss = 0.01666157\n",
      "Iteration 609, loss = 0.01669220\n",
      "Iteration 610, loss = 0.01659600\n",
      "Iteration 611, loss = 0.01659736\n",
      "Iteration 612, loss = 0.01652529\n",
      "Iteration 613, loss = 0.01645522\n",
      "Iteration 614, loss = 0.01642736\n",
      "Iteration 615, loss = 0.01643353\n",
      "Iteration 616, loss = 0.01630584\n",
      "Iteration 617, loss = 0.01632351\n",
      "Iteration 618, loss = 0.01631345\n",
      "Iteration 619, loss = 0.01620100\n",
      "Iteration 620, loss = 0.01636391\n",
      "Iteration 621, loss = 0.01619275\n",
      "Iteration 622, loss = 0.01612767\n",
      "Iteration 623, loss = 0.01610077\n",
      "Iteration 624, loss = 0.01605879\n",
      "Iteration 625, loss = 0.01601106\n",
      "Iteration 626, loss = 0.01595540\n",
      "Iteration 627, loss = 0.01591645\n",
      "Iteration 628, loss = 0.01587526\n",
      "Iteration 629, loss = 0.01587304\n",
      "Iteration 630, loss = 0.01579106\n",
      "Iteration 631, loss = 0.01574843\n",
      "Iteration 632, loss = 0.01575752\n",
      "Iteration 633, loss = 0.01570517\n",
      "Iteration 634, loss = 0.01570363\n",
      "Iteration 635, loss = 0.01561873\n",
      "Iteration 636, loss = 0.01552927\n",
      "Iteration 637, loss = 0.01556432\n",
      "Iteration 638, loss = 0.01552848\n",
      "Iteration 639, loss = 0.01551787\n",
      "Iteration 640, loss = 0.01540183\n",
      "Iteration 641, loss = 0.01539944\n",
      "Iteration 642, loss = 0.01533107\n",
      "Iteration 643, loss = 0.01527367\n",
      "Iteration 644, loss = 0.01526863\n",
      "Iteration 645, loss = 0.01525080\n",
      "Iteration 646, loss = 0.01518749\n",
      "Iteration 647, loss = 0.01516728\n",
      "Iteration 648, loss = 0.01507786\n",
      "Iteration 649, loss = 0.01506792\n",
      "Iteration 650, loss = 0.01501446\n",
      "Iteration 651, loss = 0.01500585\n",
      "Iteration 652, loss = 0.01496115\n",
      "Iteration 653, loss = 0.01490072\n",
      "Iteration 654, loss = 0.01490982\n",
      "Iteration 655, loss = 0.01484919\n",
      "Iteration 656, loss = 0.01481508\n",
      "Iteration 657, loss = 0.01474762\n",
      "Iteration 658, loss = 0.01475838\n",
      "Iteration 659, loss = 0.01472079\n",
      "Iteration 660, loss = 0.01466880\n",
      "Iteration 661, loss = 0.01462551\n",
      "Iteration 662, loss = 0.01467981\n",
      "Iteration 663, loss = 0.01455683\n",
      "Iteration 664, loss = 0.01451255\n",
      "Iteration 665, loss = 0.01445894\n",
      "Iteration 666, loss = 0.01443458\n",
      "Iteration 667, loss = 0.01439631\n",
      "Iteration 668, loss = 0.01440013\n",
      "Iteration 669, loss = 0.01437895\n",
      "Iteration 670, loss = 0.01440949\n",
      "Iteration 671, loss = 0.01423425\n",
      "Iteration 672, loss = 0.01428989\n",
      "Iteration 673, loss = 0.01424833\n",
      "Iteration 674, loss = 0.01423268\n",
      "Iteration 675, loss = 0.01418442\n",
      "Iteration 676, loss = 0.01412234\n",
      "Iteration 677, loss = 0.01407204\n",
      "Iteration 678, loss = 0.01406434\n",
      "Iteration 679, loss = 0.01403666\n",
      "Iteration 680, loss = 0.01399904\n",
      "Iteration 681, loss = 0.01395251\n",
      "Iteration 682, loss = 0.01391799\n",
      "Iteration 683, loss = 0.01393519\n",
      "Iteration 684, loss = 0.01388676\n",
      "Iteration 685, loss = 0.01383634\n",
      "Iteration 686, loss = 0.01379442\n",
      "Iteration 687, loss = 0.01373345\n",
      "Iteration 688, loss = 0.01373463\n",
      "Iteration 689, loss = 0.01367077\n",
      "Iteration 690, loss = 0.01366746\n",
      "Iteration 691, loss = 0.01364263\n",
      "Iteration 692, loss = 0.01359952\n",
      "Iteration 693, loss = 0.01357139\n",
      "Iteration 694, loss = 0.01352977\n",
      "Iteration 695, loss = 0.01354153\n",
      "Iteration 696, loss = 0.01347722\n",
      "Iteration 697, loss = 0.01344507\n",
      "Iteration 698, loss = 0.01340570\n",
      "Iteration 699, loss = 0.01338919\n",
      "Iteration 700, loss = 0.01341052\n",
      "Iteration 701, loss = 0.01333107\n",
      "Iteration 702, loss = 0.01331343\n",
      "Iteration 703, loss = 0.01329720\n",
      "Iteration 704, loss = 0.01324972\n",
      "Iteration 705, loss = 0.01323864\n",
      "Iteration 706, loss = 0.01318365\n",
      "Iteration 707, loss = 0.01314814\n",
      "Iteration 708, loss = 0.01311336\n",
      "Iteration 709, loss = 0.01309149\n",
      "Iteration 710, loss = 0.01305767\n",
      "Iteration 711, loss = 0.01303520\n",
      "Iteration 712, loss = 0.01303387\n",
      "Iteration 713, loss = 0.01297721\n",
      "Iteration 714, loss = 0.01296779\n",
      "Iteration 715, loss = 0.01292688\n",
      "Iteration 716, loss = 0.01293376\n",
      "Iteration 717, loss = 0.01286859\n",
      "Iteration 718, loss = 0.01283180\n",
      "Iteration 719, loss = 0.01290330\n",
      "Iteration 720, loss = 0.01281227\n",
      "Iteration 721, loss = 0.01282761\n",
      "Iteration 722, loss = 0.01272315\n",
      "Iteration 723, loss = 0.01268908\n",
      "Iteration 724, loss = 0.01267972\n",
      "Iteration 725, loss = 0.01263403\n",
      "Iteration 726, loss = 0.01263728\n",
      "Iteration 727, loss = 0.01261009\n",
      "Iteration 728, loss = 0.01259042\n",
      "Iteration 729, loss = 0.01253311\n",
      "Iteration 730, loss = 0.01252535\n",
      "Iteration 731, loss = 0.01249492\n",
      "Iteration 732, loss = 0.01251419\n",
      "Iteration 733, loss = 0.01244924\n",
      "Iteration 734, loss = 0.01243558\n",
      "Iteration 735, loss = 0.01240093\n",
      "Iteration 736, loss = 0.01236701\n",
      "Iteration 737, loss = 0.01233037\n",
      "Iteration 738, loss = 0.01230678\n",
      "Iteration 739, loss = 0.01230008\n",
      "Iteration 740, loss = 0.01226571\n",
      "Iteration 741, loss = 0.01221930\n",
      "Iteration 742, loss = 0.01220045\n",
      "Iteration 743, loss = 0.01217759\n",
      "Iteration 744, loss = 0.01217992\n",
      "Iteration 745, loss = 0.01213958\n",
      "Iteration 746, loss = 0.01210510\n",
      "Iteration 747, loss = 0.01209004\n",
      "Iteration 748, loss = 0.01207768\n",
      "Iteration 749, loss = 0.01207932\n",
      "Iteration 750, loss = 0.01205370\n",
      "Iteration 751, loss = 0.01204986\n",
      "Iteration 752, loss = 0.01193738\n",
      "Iteration 753, loss = 0.01193826\n",
      "Iteration 754, loss = 0.01194645\n",
      "Iteration 755, loss = 0.01187078\n",
      "Iteration 756, loss = 0.01184374\n",
      "Iteration 757, loss = 0.01183085\n",
      "Iteration 758, loss = 0.01181567\n",
      "Iteration 759, loss = 0.01179829\n",
      "Iteration 760, loss = 0.01181304\n",
      "Iteration 761, loss = 0.01179315\n",
      "Iteration 762, loss = 0.01170350\n",
      "Iteration 763, loss = 0.01168115\n",
      "Iteration 764, loss = 0.01169948\n",
      "Iteration 765, loss = 0.01164692\n",
      "Iteration 766, loss = 0.01162724\n",
      "Iteration 767, loss = 0.01159053\n",
      "Iteration 768, loss = 0.01158390\n",
      "Iteration 769, loss = 0.01154019\n",
      "Iteration 770, loss = 0.01152427\n",
      "Iteration 771, loss = 0.01151109\n",
      "Iteration 772, loss = 0.01144907\n",
      "Iteration 773, loss = 0.01142813\n",
      "Iteration 774, loss = 0.01144808\n",
      "Iteration 775, loss = 0.01147764\n",
      "Iteration 776, loss = 0.01139027\n",
      "Iteration 777, loss = 0.01136725\n",
      "Iteration 778, loss = 0.01135047\n",
      "Iteration 779, loss = 0.01133471\n",
      "Iteration 780, loss = 0.01128385\n",
      "Iteration 781, loss = 0.01128257\n",
      "Iteration 782, loss = 0.01125663\n",
      "Iteration 783, loss = 0.01126590\n",
      "Iteration 784, loss = 0.01119745\n",
      "Iteration 785, loss = 0.01121427\n",
      "Iteration 786, loss = 0.01122275\n",
      "Iteration 787, loss = 0.01114870\n",
      "Iteration 788, loss = 0.01110755\n",
      "Iteration 789, loss = 0.01109386\n",
      "Iteration 790, loss = 0.01108512\n",
      "Iteration 791, loss = 0.01106280\n",
      "Iteration 792, loss = 0.01104548\n",
      "Iteration 793, loss = 0.01101636\n",
      "Iteration 794, loss = 0.01097994\n",
      "Iteration 795, loss = 0.01098866\n",
      "Iteration 796, loss = 0.01096690\n",
      "Iteration 797, loss = 0.01092765\n",
      "Iteration 798, loss = 0.01091204\n",
      "Iteration 799, loss = 0.01087947\n",
      "Iteration 800, loss = 0.01086084\n",
      "Iteration 801, loss = 0.01086635\n",
      "Iteration 802, loss = 0.01088688\n",
      "Iteration 803, loss = 0.01079293\n",
      "Iteration 804, loss = 0.01077820\n",
      "Iteration 805, loss = 0.01076716\n",
      "Iteration 806, loss = 0.01074925\n",
      "Iteration 807, loss = 0.01071332\n",
      "Iteration 808, loss = 0.01070134\n",
      "Iteration 809, loss = 0.01070214\n",
      "Iteration 810, loss = 0.01068123\n",
      "Iteration 811, loss = 0.01062480\n",
      "Iteration 812, loss = 0.01063666\n",
      "Iteration 813, loss = 0.01058071\n",
      "Iteration 814, loss = 0.01063793\n",
      "Iteration 815, loss = 0.01052957\n",
      "Iteration 816, loss = 0.01066750\n",
      "Iteration 817, loss = 0.01054722\n",
      "Iteration 818, loss = 0.01047882\n",
      "Iteration 819, loss = 0.01049963\n",
      "Iteration 820, loss = 0.01047354\n",
      "Iteration 821, loss = 0.01042475\n",
      "Iteration 822, loss = 0.01039110\n",
      "Iteration 823, loss = 0.01039196\n",
      "Iteration 824, loss = 0.01039930\n",
      "Iteration 825, loss = 0.01038729\n",
      "Iteration 826, loss = 0.01036956\n",
      "Iteration 827, loss = 0.01033023\n",
      "Iteration 828, loss = 0.01029243\n",
      "Iteration 829, loss = 0.01030630\n",
      "Iteration 830, loss = 0.01038500\n",
      "Iteration 831, loss = 0.01026049\n",
      "Iteration 832, loss = 0.01020016\n",
      "Iteration 833, loss = 0.01018883\n",
      "Iteration 834, loss = 0.01017605\n",
      "Iteration 835, loss = 0.01019646\n",
      "Iteration 836, loss = 0.01016083\n",
      "Iteration 837, loss = 0.01014008\n",
      "Iteration 838, loss = 0.01013071\n",
      "Iteration 839, loss = 0.01008019\n",
      "Iteration 840, loss = 0.01005806\n",
      "Iteration 841, loss = 0.01009039\n",
      "Iteration 842, loss = 0.01003776\n",
      "Iteration 843, loss = 0.01006013\n",
      "Iteration 844, loss = 0.01004166\n",
      "Iteration 845, loss = 0.00998877\n",
      "Iteration 846, loss = 0.00993831\n",
      "Iteration 847, loss = 0.00993892\n",
      "Iteration 848, loss = 0.00991508\n",
      "Iteration 849, loss = 0.00990157\n",
      "Iteration 850, loss = 0.00986808\n",
      "Iteration 851, loss = 0.00989862\n",
      "Iteration 852, loss = 0.00987576\n",
      "Iteration 853, loss = 0.00996144\n",
      "Iteration 854, loss = 0.00986583\n",
      "Iteration 855, loss = 0.00989623\n",
      "Iteration 856, loss = 0.00977498\n",
      "Iteration 857, loss = 0.00989885\n",
      "Iteration 858, loss = 0.00974998\n",
      "Iteration 859, loss = 0.00978378\n",
      "Iteration 860, loss = 0.00972682\n",
      "Iteration 861, loss = 0.00967498\n",
      "Iteration 862, loss = 0.00967806\n",
      "Iteration 863, loss = 0.00969769\n",
      "Iteration 864, loss = 0.00964582\n",
      "Iteration 865, loss = 0.00967841\n",
      "Iteration 866, loss = 0.00962872\n",
      "Iteration 867, loss = 0.00958888\n",
      "Iteration 868, loss = 0.00955583\n",
      "Iteration 869, loss = 0.00959620\n",
      "Iteration 870, loss = 0.00954429\n",
      "Iteration 871, loss = 0.00950766\n",
      "Iteration 872, loss = 0.00949431\n",
      "Iteration 873, loss = 0.00946342\n",
      "Iteration 874, loss = 0.00948146\n",
      "Iteration 875, loss = 0.00943308\n",
      "Iteration 876, loss = 0.00941461\n",
      "Iteration 877, loss = 0.00942647\n",
      "Iteration 878, loss = 0.00940298\n",
      "Iteration 879, loss = 0.00936214\n",
      "Iteration 880, loss = 0.00934114\n",
      "Iteration 881, loss = 0.00933862\n",
      "Iteration 882, loss = 0.00933658\n",
      "Iteration 883, loss = 0.00933702\n",
      "Iteration 884, loss = 0.00931423\n",
      "Iteration 885, loss = 0.00928953\n",
      "Iteration 886, loss = 0.00925792\n",
      "Iteration 887, loss = 0.00924999\n",
      "Iteration 888, loss = 0.00923361\n",
      "Iteration 889, loss = 0.00920619\n",
      "Iteration 890, loss = 0.00923248\n",
      "Iteration 891, loss = 0.00925998\n",
      "Iteration 892, loss = 0.00917717\n",
      "Iteration 893, loss = 0.00916671\n",
      "Iteration 894, loss = 0.00916577\n",
      "Iteration 895, loss = 0.00913570\n",
      "Iteration 896, loss = 0.00910763\n",
      "Iteration 897, loss = 0.00909372\n",
      "Iteration 898, loss = 0.00907255\n",
      "Iteration 899, loss = 0.00906724\n",
      "Iteration 900, loss = 0.00903022\n",
      "Iteration 901, loss = 0.00904407\n",
      "Iteration 902, loss = 0.00903673\n",
      "Iteration 903, loss = 0.00899916\n",
      "Iteration 904, loss = 0.00899681\n",
      "Iteration 905, loss = 0.00897720\n",
      "Iteration 906, loss = 0.00896037\n",
      "Iteration 907, loss = 0.00894982\n",
      "Iteration 908, loss = 0.00891177\n",
      "Iteration 909, loss = 0.00890278\n",
      "Iteration 910, loss = 0.00890498\n",
      "Iteration 911, loss = 0.00887460\n",
      "Iteration 912, loss = 0.00888059\n",
      "Iteration 913, loss = 0.00886789\n",
      "Iteration 914, loss = 0.00883918\n",
      "Iteration 915, loss = 0.00884112\n",
      "Iteration 916, loss = 0.00881524\n",
      "Iteration 917, loss = 0.00882123\n",
      "Iteration 918, loss = 0.00877346\n",
      "Iteration 919, loss = 0.00875877\n",
      "Iteration 920, loss = 0.00875391\n",
      "Iteration 921, loss = 0.00874368\n",
      "Iteration 922, loss = 0.00874074\n",
      "Iteration 923, loss = 0.00869577\n",
      "Iteration 924, loss = 0.00868762\n",
      "Iteration 925, loss = 0.00865694\n",
      "Iteration 926, loss = 0.00866650\n",
      "Iteration 927, loss = 0.00868942\n",
      "Iteration 928, loss = 0.00864510\n",
      "Iteration 929, loss = 0.00864821\n",
      "Iteration 930, loss = 0.00864083\n",
      "Iteration 931, loss = 0.00858476\n",
      "Iteration 932, loss = 0.00857491\n",
      "Iteration 933, loss = 0.00857268\n",
      "Iteration 934, loss = 0.00854739\n",
      "Iteration 935, loss = 0.00858236\n",
      "Iteration 936, loss = 0.00851183\n",
      "Iteration 937, loss = 0.00854891\n",
      "Iteration 938, loss = 0.00854146\n",
      "Iteration 939, loss = 0.00848323\n",
      "Iteration 940, loss = 0.00845456\n",
      "Iteration 941, loss = 0.00845507\n",
      "Iteration 942, loss = 0.00846592\n",
      "Iteration 943, loss = 0.00843396\n",
      "Iteration 944, loss = 0.00839319\n",
      "Iteration 945, loss = 0.00840289\n",
      "Iteration 946, loss = 0.00839467\n",
      "Iteration 947, loss = 0.00837025\n",
      "Iteration 948, loss = 0.00835567\n",
      "Iteration 949, loss = 0.00837789\n",
      "Iteration 950, loss = 0.00832821\n",
      "Iteration 951, loss = 0.00831337\n",
      "Iteration 952, loss = 0.00829895\n",
      "Iteration 953, loss = 0.00831232\n",
      "Iteration 954, loss = 0.00830609\n",
      "Iteration 955, loss = 0.00825388\n",
      "Iteration 956, loss = 0.00820896\n",
      "Iteration 957, loss = 0.00827746\n",
      "Iteration 958, loss = 0.00823497\n",
      "Iteration 959, loss = 0.00821490\n",
      "Iteration 960, loss = 0.00821455\n",
      "Iteration 961, loss = 0.00817117\n",
      "Iteration 962, loss = 0.00816282\n",
      "Iteration 963, loss = 0.00816423\n",
      "Iteration 964, loss = 0.00815300\n",
      "Iteration 965, loss = 0.00825845\n",
      "Iteration 966, loss = 0.00817479\n",
      "Iteration 967, loss = 0.00811360\n",
      "Iteration 968, loss = 0.00820827\n",
      "Iteration 969, loss = 0.00810614\n",
      "Iteration 970, loss = 0.00805852\n",
      "Iteration 971, loss = 0.00804310\n",
      "Iteration 972, loss = 0.00808852\n",
      "Iteration 973, loss = 0.00800173\n",
      "Iteration 974, loss = 0.00811297\n",
      "Iteration 975, loss = 0.00804947\n",
      "Iteration 976, loss = 0.00799348\n",
      "Iteration 977, loss = 0.00800493\n",
      "Iteration 978, loss = 0.00794509\n",
      "Iteration 979, loss = 0.00793920\n",
      "Iteration 980, loss = 0.00793478\n",
      "Iteration 981, loss = 0.00794971\n",
      "Iteration 982, loss = 0.00788951\n",
      "Iteration 983, loss = 0.00789568\n",
      "Iteration 984, loss = 0.00787847\n",
      "Iteration 985, loss = 0.00791708\n",
      "Iteration 986, loss = 0.00789931\n",
      "Iteration 987, loss = 0.00790669\n",
      "Iteration 988, loss = 0.00780265\n",
      "Iteration 989, loss = 0.00793175\n",
      "Iteration 990, loss = 0.00784534\n",
      "Iteration 991, loss = 0.00780717\n",
      "Iteration 992, loss = 0.00780768\n",
      "Iteration 993, loss = 0.00777733\n",
      "Iteration 994, loss = 0.00777402\n",
      "Iteration 995, loss = 0.00773775\n",
      "Iteration 996, loss = 0.00772184\n",
      "Iteration 997, loss = 0.00773955\n",
      "Iteration 998, loss = 0.00771429\n",
      "Iteration 999, loss = 0.00771300\n",
      "Iteration 1000, loss = 0.00769282\n",
      "Iteration 1001, loss = 0.00766997\n",
      "Iteration 1002, loss = 0.00767204\n",
      "Iteration 1003, loss = 0.00765529\n",
      "Iteration 1004, loss = 0.00762817\n",
      "Iteration 1005, loss = 0.00768239\n",
      "Iteration 1006, loss = 0.00762149\n",
      "Iteration 1007, loss = 0.00763195\n",
      "Iteration 1008, loss = 0.00761460\n",
      "Iteration 1009, loss = 0.00761312\n",
      "Iteration 1010, loss = 0.00757585\n",
      "Iteration 1011, loss = 0.00757105\n",
      "Iteration 1012, loss = 0.00756868\n",
      "Iteration 1013, loss = 0.00751364\n",
      "Iteration 1014, loss = 0.00755261\n",
      "Iteration 1015, loss = 0.00750286\n",
      "Iteration 1016, loss = 0.00750285\n",
      "Iteration 1017, loss = 0.00748944\n",
      "Iteration 1018, loss = 0.00748434\n",
      "Iteration 1019, loss = 0.00746472\n",
      "Iteration 1020, loss = 0.00747393\n",
      "Iteration 1021, loss = 0.00745037\n",
      "Iteration 1022, loss = 0.00744564\n",
      "Iteration 1023, loss = 0.00741656\n",
      "Iteration 1024, loss = 0.00745495\n",
      "Iteration 1025, loss = 0.00751530\n",
      "Iteration 1026, loss = 0.00739679\n",
      "Iteration 1027, loss = 0.00737585\n",
      "Iteration 1028, loss = 0.00739986\n",
      "Iteration 1029, loss = 0.00734896\n",
      "Iteration 1030, loss = 0.00737411\n",
      "Iteration 1031, loss = 0.00737688\n",
      "Iteration 1032, loss = 0.00732735\n",
      "Iteration 1033, loss = 0.00734551\n",
      "Iteration 1034, loss = 0.00730173\n",
      "Iteration 1035, loss = 0.00731071\n",
      "Iteration 1036, loss = 0.00726769\n",
      "Iteration 1037, loss = 0.00728320\n",
      "Iteration 1038, loss = 0.00727868\n",
      "Iteration 1039, loss = 0.00724909\n",
      "Iteration 1040, loss = 0.00728117\n",
      "Iteration 1041, loss = 0.00722001\n",
      "Iteration 1042, loss = 0.00725196\n",
      "Iteration 1043, loss = 0.00729891\n",
      "Iteration 1044, loss = 0.00722085\n",
      "Iteration 1045, loss = 0.00717127\n",
      "Iteration 1046, loss = 0.00720397\n",
      "Iteration 1047, loss = 0.00717626\n",
      "Iteration 1048, loss = 0.00716129\n",
      "Iteration 1049, loss = 0.00711608\n",
      "Iteration 1050, loss = 0.00719751\n",
      "Iteration 1051, loss = 0.00713919\n",
      "Iteration 1052, loss = 0.00714767\n",
      "Iteration 1053, loss = 0.00713648\n",
      "Iteration 1054, loss = 0.00710063\n",
      "Iteration 1055, loss = 0.00710878\n",
      "Iteration 1056, loss = 0.00711066\n",
      "Iteration 1057, loss = 0.00717558\n",
      "Iteration 1058, loss = 0.00716403\n",
      "Iteration 1059, loss = 0.00708441\n",
      "Iteration 1060, loss = 0.00702047\n",
      "Iteration 1061, loss = 0.00705059\n",
      "Iteration 1062, loss = 0.00703385\n",
      "Iteration 1063, loss = 0.00705379\n",
      "Iteration 1064, loss = 0.00698251\n",
      "Iteration 1065, loss = 0.00699077\n",
      "Iteration 1066, loss = 0.00697899\n",
      "Iteration 1067, loss = 0.00702938\n",
      "Iteration 1068, loss = 0.00699187\n",
      "Iteration 1069, loss = 0.00695144\n",
      "Iteration 1070, loss = 0.00697110\n",
      "Iteration 1071, loss = 0.00691413\n",
      "Iteration 1072, loss = 0.00694213\n",
      "Iteration 1073, loss = 0.00696168\n",
      "Iteration 1074, loss = 0.00689173\n",
      "Iteration 1075, loss = 0.00696143\n",
      "Iteration 1076, loss = 0.00690421\n",
      "Iteration 1077, loss = 0.00688676\n",
      "Iteration 1078, loss = 0.00687161\n",
      "Iteration 1079, loss = 0.00693826\n",
      "Iteration 1080, loss = 0.00683041\n",
      "Iteration 1081, loss = 0.00681642\n",
      "Iteration 1082, loss = 0.00682827\n",
      "Iteration 1083, loss = 0.00682653\n",
      "Iteration 1084, loss = 0.00685586\n",
      "Iteration 1085, loss = 0.00677819\n",
      "Iteration 1086, loss = 0.00676443\n",
      "Iteration 1087, loss = 0.00677608\n",
      "Iteration 1088, loss = 0.00677217\n",
      "Iteration 1089, loss = 0.00674935\n",
      "Iteration 1090, loss = 0.00675965\n",
      "Iteration 1091, loss = 0.00673792\n",
      "Iteration 1092, loss = 0.00672725\n",
      "Iteration 1093, loss = 0.00671555\n",
      "Iteration 1094, loss = 0.00671688\n",
      "Iteration 1095, loss = 0.00671135\n",
      "Iteration 1096, loss = 0.00674417\n",
      "Iteration 1097, loss = 0.00674553\n",
      "Iteration 1098, loss = 0.00667869\n",
      "Iteration 1099, loss = 0.00668274\n",
      "Iteration 1100, loss = 0.00665614\n",
      "Iteration 1101, loss = 0.00663810\n",
      "Iteration 1102, loss = 0.00661627\n",
      "Iteration 1103, loss = 0.00664706\n",
      "Iteration 1104, loss = 0.00661583\n",
      "Iteration 1105, loss = 0.00659113\n",
      "Iteration 1106, loss = 0.00661678\n",
      "Iteration 1107, loss = 0.00657310\n",
      "Iteration 1108, loss = 0.00658094\n",
      "Iteration 1109, loss = 0.00658307\n",
      "Iteration 1110, loss = 0.00656206\n",
      "Iteration 1111, loss = 0.00656760\n",
      "Iteration 1112, loss = 0.00656253\n",
      "Iteration 1113, loss = 0.00654211\n",
      "Iteration 1114, loss = 0.00653247\n",
      "Iteration 1115, loss = 0.00651480\n",
      "Iteration 1116, loss = 0.00650089\n",
      "Iteration 1117, loss = 0.00650561\n",
      "Iteration 1118, loss = 0.00650382\n",
      "Iteration 1119, loss = 0.00650175\n",
      "Iteration 1120, loss = 0.00651189\n",
      "Iteration 1121, loss = 0.00647151\n",
      "Iteration 1122, loss = 0.00648095\n",
      "Iteration 1123, loss = 0.00647463\n",
      "Iteration 1124, loss = 0.00642741\n",
      "Iteration 1125, loss = 0.00646584\n",
      "Iteration 1126, loss = 0.00643686\n",
      "Iteration 1127, loss = 0.00653192\n",
      "Iteration 1128, loss = 0.00649311\n",
      "Iteration 1129, loss = 0.00642046\n",
      "Iteration 1130, loss = 0.00639242\n",
      "Iteration 1131, loss = 0.00641948\n",
      "Iteration 1132, loss = 0.00635709\n",
      "Iteration 1133, loss = 0.00649434\n",
      "Iteration 1134, loss = 0.00639905\n",
      "Iteration 1135, loss = 0.00639641\n",
      "Iteration 1136, loss = 0.00636074\n",
      "Iteration 1137, loss = 0.00633700\n",
      "Iteration 1138, loss = 0.00631610\n",
      "Iteration 1139, loss = 0.00635453\n",
      "Iteration 1140, loss = 0.00631855\n",
      "Iteration 1141, loss = 0.00632586\n",
      "Iteration 1142, loss = 0.00629383\n",
      "Iteration 1143, loss = 0.00634342\n",
      "Iteration 1144, loss = 0.00628383\n",
      "Iteration 1145, loss = 0.00626610\n",
      "Iteration 1146, loss = 0.00624956\n",
      "Iteration 1147, loss = 0.00625158\n",
      "Iteration 1148, loss = 0.00629367\n",
      "Iteration 1149, loss = 0.00626370\n",
      "Iteration 1150, loss = 0.00622280\n",
      "Iteration 1151, loss = 0.00623603\n",
      "Iteration 1152, loss = 0.00628099\n",
      "Iteration 1153, loss = 0.00617807\n",
      "Iteration 1154, loss = 0.00623843\n",
      "Iteration 1155, loss = 0.00620877\n",
      "Iteration 1156, loss = 0.00619539\n",
      "Iteration 1157, loss = 0.00615865\n",
      "Iteration 1158, loss = 0.00616424\n",
      "Iteration 1159, loss = 0.00615704\n",
      "Iteration 1160, loss = 0.00619404\n",
      "Iteration 1161, loss = 0.00614473\n",
      "Iteration 1162, loss = 0.00611528\n",
      "Iteration 1163, loss = 0.00611275\n",
      "Iteration 1164, loss = 0.00617500\n",
      "Iteration 1165, loss = 0.00613098\n",
      "Iteration 1166, loss = 0.00609518\n",
      "Iteration 1167, loss = 0.00607564\n",
      "Iteration 1168, loss = 0.00605732\n",
      "Iteration 1169, loss = 0.00608181\n",
      "Iteration 1170, loss = 0.00606052\n",
      "Iteration 1171, loss = 0.00605004\n",
      "Iteration 1172, loss = 0.00606398\n",
      "Iteration 1173, loss = 0.00604384\n",
      "Iteration 1174, loss = 0.00604187\n",
      "Iteration 1175, loss = 0.00603345\n",
      "Iteration 1176, loss = 0.00599815\n",
      "Iteration 1177, loss = 0.00601313\n",
      "Iteration 1178, loss = 0.00602187\n",
      "Iteration 1179, loss = 0.00600008\n",
      "Iteration 1180, loss = 0.00601158\n",
      "Iteration 1181, loss = 0.00599071\n",
      "Iteration 1182, loss = 0.00605944\n",
      "Iteration 1183, loss = 0.00603958\n",
      "Iteration 1184, loss = 0.00602645\n",
      "Iteration 1185, loss = 0.00605890\n",
      "Iteration 1186, loss = 0.00595134\n",
      "Iteration 1187, loss = 0.00593413\n",
      "Iteration 1188, loss = 0.00596997\n",
      "Iteration 1189, loss = 0.00591284\n",
      "Iteration 1190, loss = 0.00591857\n",
      "Iteration 1191, loss = 0.00593270\n",
      "Iteration 1192, loss = 0.00594572\n",
      "Iteration 1193, loss = 0.00597178\n",
      "Iteration 1194, loss = 0.00587700\n",
      "Iteration 1195, loss = 0.00587561\n",
      "Iteration 1196, loss = 0.00589243\n",
      "Iteration 1197, loss = 0.00586699\n",
      "Iteration 1198, loss = 0.00585402\n",
      "Iteration 1199, loss = 0.00587772\n",
      "Iteration 1200, loss = 0.00582315\n",
      "Iteration 1201, loss = 0.00583704\n",
      "Iteration 1202, loss = 0.00581498\n",
      "Iteration 1203, loss = 0.00580858\n",
      "Iteration 1204, loss = 0.00580620\n",
      "Iteration 1205, loss = 0.00583637\n",
      "Iteration 1206, loss = 0.00582132\n",
      "Iteration 1207, loss = 0.00578904\n",
      "Iteration 1208, loss = 0.00579308\n",
      "Iteration 1209, loss = 0.00577798\n",
      "Iteration 1210, loss = 0.00582989\n",
      "Iteration 1211, loss = 0.00579189\n",
      "Iteration 1212, loss = 0.00577412\n",
      "Iteration 1213, loss = 0.00574371\n",
      "Iteration 1214, loss = 0.00574118\n",
      "Iteration 1215, loss = 0.00576219\n",
      "Iteration 1216, loss = 0.00589729\n",
      "Iteration 1217, loss = 0.00576429\n",
      "Iteration 1218, loss = 0.00569833\n",
      "Iteration 1219, loss = 0.00572018\n",
      "Iteration 1220, loss = 0.00576304\n",
      "Iteration 1221, loss = 0.00572477\n",
      "Iteration 1222, loss = 0.00579167\n",
      "Iteration 1223, loss = 0.00577333\n",
      "Iteration 1224, loss = 0.00572082\n",
      "Iteration 1225, loss = 0.00566114\n",
      "Iteration 1226, loss = 0.00571713\n",
      "Iteration 1227, loss = 0.00562579\n",
      "Iteration 1228, loss = 0.00567630\n",
      "Iteration 1229, loss = 0.00563856\n",
      "Iteration 1230, loss = 0.00567156\n",
      "Iteration 1231, loss = 0.00561326\n",
      "Iteration 1232, loss = 0.00566748\n",
      "Iteration 1233, loss = 0.00565492\n",
      "Iteration 1234, loss = 0.00560246\n",
      "Iteration 1235, loss = 0.00570619\n",
      "Iteration 1236, loss = 0.00562081\n",
      "Iteration 1237, loss = 0.00556210\n",
      "Iteration 1238, loss = 0.00561380\n",
      "Iteration 1239, loss = 0.00558341\n",
      "Iteration 1240, loss = 0.00562155\n",
      "Iteration 1241, loss = 0.00562479\n",
      "Iteration 1242, loss = 0.00558464\n",
      "Iteration 1243, loss = 0.00554339\n",
      "Iteration 1244, loss = 0.00554878\n",
      "Iteration 1245, loss = 0.00552932\n",
      "Iteration 1246, loss = 0.00556515\n",
      "Iteration 1247, loss = 0.00554986\n",
      "Iteration 1248, loss = 0.00553054\n",
      "Iteration 1249, loss = 0.00550037\n",
      "Iteration 1250, loss = 0.00555504\n",
      "Iteration 1251, loss = 0.00554910\n",
      "Iteration 1252, loss = 0.00550223\n",
      "Iteration 1253, loss = 0.00554081\n",
      "Iteration 1254, loss = 0.00545140\n",
      "Iteration 1255, loss = 0.00549536\n",
      "Iteration 1256, loss = 0.00545770\n",
      "Iteration 1257, loss = 0.00547514\n",
      "Iteration 1258, loss = 0.00545726\n",
      "Iteration 1259, loss = 0.00553451\n",
      "Iteration 1260, loss = 0.00544925\n",
      "Iteration 1261, loss = 0.00546727\n",
      "Iteration 1262, loss = 0.00546457\n",
      "Iteration 1263, loss = 0.00542165\n",
      "Iteration 1264, loss = 0.00541938\n",
      "Iteration 1265, loss = 0.00541352\n",
      "Iteration 1266, loss = 0.00543554\n",
      "Iteration 1267, loss = 0.00539121\n",
      "Iteration 1268, loss = 0.00539980\n",
      "Iteration 1269, loss = 0.00542766\n",
      "Iteration 1270, loss = 0.00536933\n",
      "Iteration 1271, loss = 0.00536999\n",
      "Iteration 1272, loss = 0.00537481\n",
      "Iteration 1273, loss = 0.00534632\n",
      "Iteration 1274, loss = 0.00533680\n",
      "Iteration 1275, loss = 0.00536213\n",
      "Iteration 1276, loss = 0.00532177\n",
      "Iteration 1277, loss = 0.00535305\n",
      "Iteration 1278, loss = 0.00534655\n",
      "Iteration 1279, loss = 0.00531206\n",
      "Iteration 1280, loss = 0.00532263\n",
      "Iteration 1281, loss = 0.00530507\n",
      "Iteration 1282, loss = 0.00532771\n",
      "Iteration 1283, loss = 0.00530892\n",
      "Iteration 1284, loss = 0.00527546\n",
      "Iteration 1285, loss = 0.00527828\n",
      "Iteration 1286, loss = 0.00527974\n",
      "Iteration 1287, loss = 0.00534516\n",
      "Iteration 1288, loss = 0.00525162\n",
      "Iteration 1289, loss = 0.00532760\n",
      "Iteration 1290, loss = 0.00524098\n",
      "Iteration 1291, loss = 0.00533257\n",
      "Iteration 1292, loss = 0.00526827\n",
      "Iteration 1293, loss = 0.00522175\n",
      "Iteration 1294, loss = 0.00525418\n",
      "Iteration 1295, loss = 0.00535203\n",
      "Iteration 1296, loss = 0.00527380\n",
      "Iteration 1297, loss = 0.00522595\n",
      "Iteration 1298, loss = 0.00523083\n",
      "Iteration 1299, loss = 0.00526360\n",
      "Iteration 1300, loss = 0.00519155\n",
      "Iteration 1301, loss = 0.00517466\n",
      "Iteration 1302, loss = 0.00518186\n",
      "Iteration 1303, loss = 0.00517833\n",
      "Iteration 1304, loss = 0.00519666\n",
      "Iteration 1305, loss = 0.00517345\n",
      "Iteration 1306, loss = 0.00518843\n",
      "Iteration 1307, loss = 0.00513542\n",
      "Iteration 1308, loss = 0.00513615\n",
      "Iteration 1309, loss = 0.00517530\n",
      "Iteration 1310, loss = 0.00515524\n",
      "Iteration 1311, loss = 0.00513925\n",
      "Iteration 1312, loss = 0.00517242\n",
      "Iteration 1313, loss = 0.00515303\n",
      "Iteration 1314, loss = 0.00513777\n",
      "Iteration 1315, loss = 0.00513377\n",
      "Iteration 1316, loss = 0.00515067\n",
      "Iteration 1317, loss = 0.00509616\n",
      "Iteration 1318, loss = 0.00510849\n",
      "Iteration 1319, loss = 0.00513903\n",
      "Iteration 1320, loss = 0.00507801\n",
      "Iteration 1321, loss = 0.00509448\n",
      "Iteration 1322, loss = 0.00509958\n",
      "Iteration 1323, loss = 0.00505675\n",
      "Iteration 1324, loss = 0.00505431\n",
      "Iteration 1325, loss = 0.00508233\n",
      "Iteration 1326, loss = 0.00509448\n",
      "Iteration 1327, loss = 0.00508060\n",
      "Iteration 1328, loss = 0.00504916\n",
      "Iteration 1329, loss = 0.00500776\n",
      "Iteration 1330, loss = 0.00506232\n",
      "Iteration 1331, loss = 0.00502342\n",
      "Iteration 1332, loss = 0.00502000\n",
      "Iteration 1333, loss = 0.00500914\n",
      "Iteration 1334, loss = 0.00504105\n",
      "Iteration 1335, loss = 0.00500097\n",
      "Iteration 1336, loss = 0.00503345\n",
      "Iteration 1337, loss = 0.00499718\n",
      "Iteration 1338, loss = 0.00496722\n",
      "Iteration 1339, loss = 0.00500364\n",
      "Iteration 1340, loss = 0.00495687\n",
      "Iteration 1341, loss = 0.00495845\n",
      "Iteration 1342, loss = 0.00498975\n",
      "Iteration 1343, loss = 0.00496056\n",
      "Iteration 1344, loss = 0.00495698\n",
      "Iteration 1345, loss = 0.00497140\n",
      "Iteration 1346, loss = 0.00496381\n",
      "Iteration 1347, loss = 0.00493845\n",
      "Iteration 1348, loss = 0.00491367\n",
      "Iteration 1349, loss = 0.00490704\n",
      "Iteration 1350, loss = 0.00490653\n",
      "Iteration 1351, loss = 0.00493148\n",
      "Iteration 1352, loss = 0.00494164\n",
      "Iteration 1353, loss = 0.00490491\n",
      "Iteration 1354, loss = 0.00489992\n",
      "Iteration 1355, loss = 0.00500198\n",
      "Iteration 1356, loss = 0.00495647\n",
      "Iteration 1357, loss = 0.00491200\n",
      "Iteration 1358, loss = 0.00491619\n",
      "Iteration 1359, loss = 0.00490330\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=2000, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=2000, tol=1e-05, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(3, 3), max_iter=2000, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "modelo = MLPClassifier(\n",
    "    hidden_layer_sizes = (3,3),  # Camadas ocultas - 3 entrada + 2 saída = 5. 5/2 = 2.5 ... 3 neuronios para camadas ocultas\n",
    "    activation = 'relu',  # Função de ativação\n",
    "    max_iter = 2000,  # Quantidade de épocas\n",
    "    tol=0.00001,  # Tolerância a mudança de erro\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Ajuste e previsão\n",
    "modelo.fit(x_previsores, y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b1de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsao = modelo.predict(x_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe3a80df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisão do modelo: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'Precisão do modelo: {accuracy_score( y_teste, previsao)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72125fd8",
   "metadata": {},
   "source": [
    "#### Visualizando resultados do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44b553ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from matplotlib.pyplot import xlabel, ylabel\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d83c94",
   "metadata": {},
   "source": [
    "#### Explicação do resultado\n",
    "\n",
    "Nesta matriz, os valores são provenientes do cruzamento entre as previsões e os valores reais do conjunto de dados. Sendo assim, pode-se chegar à seguinte conclusão:\n",
    "\n",
    "\n",
    "**Para a classe \"0\", 436 previsões estão corretas e 0 estão incorretas.**\n",
    "\n",
    "<br>\n",
    "\n",
    "**Para a classe \"1\", 64 previsões estão corretas e 0 estão incorretas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a11b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix( modelo )\n",
    "cm.fit( x_previsores, y_classes )\n",
    "cm.score( x_teste, y_teste )\n",
    "\n",
    "xlabel('Previsões');\n",
    "ylabel('Classes');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080f339",
   "metadata": {},
   "source": [
    "#### Gerando relatório de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3de912ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       1.00      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report( y_teste, previsao ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
